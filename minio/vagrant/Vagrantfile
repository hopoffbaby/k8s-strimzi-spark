# -*- mode: ruby -*-
# vi: set ft=ruby :

# Can be provisioned faster by calling "$env:VAGRANT_EXPERIMENTAL="disks"; vagrant up node<x> from multiple terminals to make parallel"

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/jammy64"
  
  config.vm.synced_folder "../data", "/vagrant_data"
  config.vm.box_download_insecure = true

  config.vm.provision "generate_ssh_key", type: "shell", inline: <<-SHELL
    if [ ! -f /vagrant_data/id_rsa ]; then
      ssh-keygen -t rsa -b 4096 -f /vagrant_data/id_rsa -q -N ""
    fi
  SHELL

  (1..1).each do |i|
    config.vm.define "vm#{i}" do |node|
      node.vm.network "private_network", ip: "10.10.10.1#{i}"
      node.vm.network "forwarded_port", guest: 9999, host: 9999, protocol: "tcp"
      node.vm.network "forwarded_port", guest: 9999, host: 9999, protocol: "udp"
      
      #node.vm.network "public_network", bridge: "PANGP Virtual Ethernet Adapter Secure"
      node.vm.hostname = "vm#{i}"

      # Export VAGRANT_EXPERIMENTAL="disks"
      node.vm.disk :disk, name: "disk-1", size: "10GB"
      node.vm.disk :disk, name: "disk-2", size: "10GB"
      node.vm.disk :disk, name: "disk-3", size: "10GB"
      node.vm.disk :disk, name: "disk-4", size: "10GB"

      node.vm.provider "virtualbox" do |vb|
        vb.gui = false
        vb.memory = "3000"
        vb.cpus = 4
      end

      node.vm.provision "prereq", type: "shell", inline: <<-SHELL
        apt-get update
        apt-get install -y iputils-ping sshpass jq
        yes vagrant | passwd root
        echo "10.10.10.11 vm1 vm1" >> /etc/hosts
        echo "10.10.10.12 vm2 vm2" >> /etc/hosts
        sudo sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config
        sudo sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/g' /etc/ssh/sshd_config
        sudo systemctl restart sshd
      SHELL

      node.vm.provision "setup_ssh_keys", type: "shell", inline: <<-SHELL
        # Copy the generated SSH key to the VM for both vagrant and root users
        mkdir -p /home/vagrant/.ssh
        cp /vagrant_data/id_rsa /home/vagrant/.ssh/id_rsa
        cp /vagrant_data/id_rsa.pub /home/vagrant/.ssh/id_rsa.pub
        cat /vagrant_data/id_rsa.pub >> /home/vagrant/.ssh/authorized_keys
        chown -R vagrant:vagrant /home/vagrant/.ssh
        chmod 600 /home/vagrant/.ssh/id_rsa /home/vagrant/.ssh/id_rsa.pub /home/vagrant/.ssh/authorized_keys

        mkdir -p /root/.ssh
        cp /vagrant_data/id_rsa /root/.ssh/id_rsa
        cp /vagrant_data/id_rsa.pub /root/.ssh/id_rsa.pub
        cat /vagrant_data/id_rsa.pub >> /root/.ssh/authorized_keys
        chmod 600 /root/.ssh/id_rsa /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys
      SHELL

      node.vm.provision "bodge_bs_firewall", type: "shell", inline: <<-SHELL
        # Add 'insecure' option to ~/.curlrc for both vagrant and root users
        # bullshit firewall issues .....
        echo insecure >> /home/vagrant/.curlrc
        echo insecure >> /root/.curlrc
      SHELL

      node.vm.provision "install_k3s", type: "shell", inline: <<-SHELL
        mkdir -p /etc/rancher/k3s/
        cp /vagrant_data/registries.yaml /etc/rancher/k3s/registries.yaml
      
        # Install k3s and kubectl on vm1 (master)
        if [ "$(hostname)" == "vm1" ]; then
          curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--node-external-ip 10.10.10.11" sh -
          # mkdir -p /home/vagrant/.kube
          # sudo cp /etc/rancher/k3s/k3s.yaml /home/vagrant/.kube/config
          # sudo chown vagrant:vagrant /home/vagrant/.kube/config
          until sudo kubectl get all -n kube-system; do
            echo "Waiting for k3s to start..."
            sleep 2
          done
        fi

        # Install k3s and kubectl on vm2 (node) and join to the cluster
        if [ "$(hostname)" != "vm1" ]; then
          until ping -c 1 10.10.10.11; do
            echo "Waiting for vm1 to start..."
            sleep 2
          done

          # while [ ! -f /var/lib/rancher/k3s/server/node-token ]; do
          #   echo "Waiting for k3s on vm1 to generate node-token..."
          #   sleep 2
          # done

          export K3S_TOKEN=$(ssh -o StrictHostKeyChecking=no -i /home/vagrant/.ssh/id_rsa root@vm1 cat /var/lib/rancher/k3s/server/node-token)
          curl -sfL https://get.k3s.io | K3S_URL=https://vm1:6443 K3S_TOKEN=$K3S_TOKEN sh -
          # sudo mkdir -p /home/vagrant/.kube
          # sshpass -p 'vagrant' scp -o StrictHostKeyChecking=no -i /home/vagrant/.ssh/id_rsa root@10.10.10.11:/etc/rancher/k3s/k3s.yaml /home/vagrant/.kube/config
          # sudo sed -i 's/127.0.0.1/10.10.10.11/' /home/vagrant/.kube/config
          # sudo chown vagrant:vagrant /home/vagrant/.kube/config
        fi
      SHELL

      # node.vm.provision "install_directpv", type: "shell", inline: <<-SHELL
      #   if [ "$(hostname)" == "vm1" ]; then
      #     # Download DirectPV plugin.
      #     release=$(curl -sfL "https://api.github.com/repos/minio/directpv/releases/latest" | awk '/tag_name/ { print substr($2, 3, length($2)-4) }')
      #     curl -fLo kubectl-directpv https://github.com/minio/directpv/releases/download/v${release}/kubectl-directpv_${release}_linux_amd64
      #     # Make the binary executable.
      #     chmod a+x kubectl-directpv
      #     # install directpv onto cluster
      #     sudo ./kubectl-directpv --kubeconfig /etc/rancher/k3s/k3s.yaml install
      #     sudo ./kubectl-directpv --kubeconfig /etc/rancher/k3s/k3s.yaml discover
      #     sudo ./kubectl-directpv --kubeconfig /etc/rancher/k3s/k3s.yaml init --dangerous drives.yaml
      #     sudo ./kubectl-directpv --kubeconfig /etc/rancher/k3s/k3s.yaml list drives
      #     sudo kubectl apply -f /vagrant_data/pvc.yaml -n directpv
      #   fi
      # SHELL
    end
  end
end
